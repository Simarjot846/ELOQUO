import streamlit as st
import sounddevice as sd
from scipy.io.wavfile import write
import os

from analyze_audio_utils import analyze_audio
from emotion_detector_utils import detect_emotion
from smile_eye_track_module import detect_smile_and_eyes_live_streamlit

# ğŸš€ Page Setup
st.set_page_config(page_title="Eloquo - Speech Coach", layout="centered")
st.title("ğŸ™ï¸ Eloquo - Your Public Speaking Coach")
st.markdown("Practice, record, and get instant AI-powered feedback on your speech.")

# ğŸ§  Initialize session state
if "webcam_analysis" not in st.session_state:
    st.session_state["webcam_analysis"] = None
if 'is_recording' not in st.session_state:
    st.session_state.is_recording = False

# ğŸ›ï¸ Recording controls
minutes = st.slider("â±ï¸ Select Recording Duration (in minutes)", 1, 5, 2)
duration = minutes * 60
fs = 44100

# ğŸ¤ Audio Recording
if not st.session_state.is_recording:
    if st.button("ğŸ¤ Start Recording"):
        st.session_state.is_recording = True
        st.info("Recording... Speak now.")
        st.session_state.audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)
else:
    if st.button("â¹ï¸ Stop Recording"):
        sd.wait()
        write("user_audio.wav", fs, st.session_state.audio)
        st.success("Recording complete! âœ…")
        st.audio("user_audio.wav")
        st.session_state.is_recording = False

        # âœ… Audio Analysis
        with st.spinner("Analyzing audio..."):
            result = analyze_audio("user_audio.wav")
            st.subheader("ğŸ“ˆ Speech Analysis")
            for key, value in result.items():
                st.write(f"**{key}:** {value}")

        # âœ… Emotion Detection
        with st.spinner("Detecting emotion..."):
            emotion_result = detect_emotion("user_audio.wav")
            if "Detected Emotion" in emotion_result:
                st.subheader("ğŸ­ Emotion Detected")
                st.success(f"You sound: {emotion_result['Detected Emotion']}")
            else:
                st.error("âŒ Could not detect emotion.")

        # âœ… Combined Feedback
        final_feedback = generate_feedback(result, emotion_result, st.session_state.webcam_analysis)
        st.subheader("ğŸ’¡ Personalized Feedback")
        for comment, quote in final_feedback:
            st.info(f"**{comment}**\n\n> _{quote}_")

# ğŸ“· Smile & Eye Contact
with st.expander("ğŸ“· Live Smile & Eye Contact Detection"):
    if st.button("Start Live Webcam Analysis"):
        st.session_state.webcam_analysis = detect_smile_and_eyes_live_streamlit()

    if st.session_state.webcam_analysis:
        if st.session_state.webcam_analysis["eye_contact"]:
            st.success("âœ… Good Eye Contact!")
        else:
            st.warning("ğŸ‘€ Try to maintain eye contact.")

        if st.session_state.webcam_analysis["smile"]:
            st.success("ğŸ˜Š You smiled! Great positivity!")
        else:
            st.warning("ğŸ˜ Try smiling more during your speech.")

# ğŸ” Reset Session
st.markdown("---")
if st.button("ğŸ” Reset Session"):
    st.session_state.clear()
    st.experimental_rerun()

# ğŸ’¡ Feedback Function
def generate_feedback(result, emotion_result, face_result):
    feedback = []

    # ğŸ—£ï¸ Speaking Speed
    speed = result.get("Speaking Speed (Beats/min)", 0)
    if speed > 160:
        feedback.append(("You're speaking too fast. Try slowing down a bit.", "â€œSlow and steady wins the race.â€"))
    elif speed < 80:
        feedback.append(("You're speaking quite slow. Add a bit more energy!", "â€œLet your energy fill the room.â€"))
    else:
        feedback.append(("Your speaking speed is great!", "â€œKeep this rhythmâ€”itâ€™s working!â€"))

    # ğŸµ Pitch
    pitch = result.get("Average Pitch (Hz)", 0)
    if pitch < 120:
        feedback.append(("Your voice is a bit flat. Try adding more vocal variation.", "â€œYour voice is your musicâ€”make it expressive.â€"))
    elif pitch > 300:
        feedback.append(("Your pitch is a bit high. Relax and ground your voice.", "â€œCalm confidence speaks loudest.â€"))
    else:
        feedback.append(("Good pitch range. Your voice sounds confident!", "â€œConfidence isnâ€™t loudâ€”itâ€™s consistent.â€"))

    # ğŸ­ Emotion
    emotion = emotion_result.get("Detected Emotion", "")
    if emotion:
        feedback.append((f"Your emotion was detected as **{emotion}**.", "â€œLet your feelings flow through your words.â€"))

    # ğŸ‘€ Eye Contact
    if face_result:
        if face_result.get("eye_contact"):
            feedback.append(("âœ… Good eye contact detected!", "â€œEyes are the windows to connection.â€"))
        else:
            feedback.append(("ğŸ‘€ Improve your eye contact for better engagement.", "â€œLook your audience in the eyesâ€”theyâ€™ll listen.â€"))

        # ğŸ˜€ Smile
        if face_result.get("smile"):
            feedback.append(("ğŸ˜Š You smiledâ€”great energy and warmth!", "â€œA smile is your superpower on stage.â€"))
        else:
            feedback.append(("ğŸ˜ Try smiling moreâ€”it boosts trust and impact.", "â€œSmileâ€”itâ€™s the shortcut to connection.â€"))

    return feedback

